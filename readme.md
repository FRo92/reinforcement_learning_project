# T茅cnicas de Reinforcement Learning aplicado al juego Snake  #

Documentaci贸n del c贸digo utilizado para desarrollar el proyecto final del curso Deep Learning Avanzado @ MIA UC, titulado "Benchmark de metodolog铆as de Aprendizaje reforzado para Snake Game"

## Integrantes: ##
- Jos茅 Antonio Lipari
- Jos茅 Francisco Mallea
- Francisca Rojas
- Damaris Saavedra

A continuaci贸n se presenta el c贸digo usado para realizar el benchmark de las 3 t茅cnicas de aprendizaje reforzado aplicado al juego Snake, cada carpeta alberga una metodolog铆a distinta, adem谩s se dejaron los archivos csv que contienen la informaci贸n de las 2000 iteraciones que se compararon y presentaron en el informe final.

## Consideraciones: ##
Se trabajaron sobre c贸digos existentes en la web, presentados a continuaci贸n:
* [Policy Gradient](https://gist.github.com/ViniTheSwan/66fd59d78e94e06e00595ae9c1748d10#file-reinforce-py)
* [deep_q_learning](https://github.com/vedantgoswami/SnakeGameAI)
* [q_learning](https://gist.github.com/jl4r1991)

Se trabaj贸 principalmente en la homologaci贸n de las condiciones del entorno para que sean comparables (tama帽o y forma del tablero, tama帽os de serpiente y manzana) dejando un tablero de 600x400 pixeles o bien 30x20 cuadrantes para cada implementaci贸n y adem谩s homologamos los movimientos de la serpiente.

### 驴C贸mo ejecutar el c贸digo?  ###
Primero se deben instalar las librer铆as necesarias (se recomienda crear un ambiente seguro) ejecutando el siguiente comando en la terminal (python -v 3.8.10):

```
pip install -r requirements.txt
```
Luego, existen instrucciones para cada implementaci贸n para iterar ~2000 veces el juego

## Deep Q Learning  ##

Es el [m茅todo](https://github.com/FRo92/reinforcement_learning_project/tree/main/deep_q_learning) m谩s actual, se basa en el q_learning tradicional, es decir, se busca resolver la ecuaci贸n de Bellman, pero se busca representar la funci贸n Q con una red neuronal. En este caso se implement贸 en pytorch una red de una capa oculta de tama帽o 256.

Esta implementaci贸n requiere tunear los par谩metros: learning rate (factor de aprendizaje), penalizaci贸n (factor de olvido) y epsilon (factor de exploraci贸n incial).

```
make deep_q_learning
```

Mientras se ejecuta el juego se desplegar谩 una ventana emergente con la interfaz del juego que permite ver el proceso de aprendizaje, los valores de cada juego quedar谩n impresos en la terminal.

## Q Learning  ##

Para el m茅todo [dql](https://github.com/FRo92/reinforcement_learning_project/tree/main/q_learning_tradicional) es el m谩s antiguo de la comparaci贸n, se basa en la soluci贸n de la ecuaci贸n de Bellman:
$ Q(s,a) = R(s,a) + \lambda max_{a'\in A}Q(s',a') $
Donde Q(s,a) es el valor que se busca llenar de forma tabular en funci贸n de estados s y acciones a, $\lambda$ es el factor de olvido de recompensas lejanas y $max_{a'\in A}Q(s',a')$ es la acci贸n que maximiza la recompensa futura.

Esta implementaci贸n requiere ajustar los par谩metros $\lambda$ y el factor de exploraci贸n $\epsilon$.

Para ejecutar este m茅todo, debemos primero debemos inicializar los valores de Q y luego ejecutar el agente con la siguiente instrucci贸n:
```
make q_learning
```
Mientras se ejecuta el juego se desplegar谩 una ventana emergente con la interfaz del juego que permite ver el proceso de aprendizaje, los valores de cada juego quedar谩n impresos en la terminal.

## Policy Gradient  ##

El m茅todo [policyGradient](https://github.com/FRo92/reinforcement_learning_project/tree/main/policy_gradient), a diferencia de los m茅todos anteriores, busca aprender la pol铆tica directamente como una funci贸n de probabilidad, en este caso, discreta usando [tfp.distributions.Categorical](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Categorical). Adem谩s, la pol铆tica se actualiza cada vez que la serpiente muere. Se busca maximizar la probabilidad de aquellas acciones que aumentan la recompenza y minimizar aquellas que la disminuyen, lo que se traduce en la siguiente funci贸n de p茅rdida:

$ \ell(\theta) = -log \pi(a_t | s_t)G_t $

Donde $\pi$ es la pol铆tica y $G_t$ es la recompensa total.
Aplicando el descenso de gradiente a la funci贸n de p茅rdida se obtiene:

$ \theta \leftarrow \theta + \eta \nabla log \pi(a_t | s_t)G_t $

Esta implementaci贸n est谩 hecha en tenssorflow con dos capas densas de tama帽o 30.

Para el m茅todo dql ejecutamos la instrucci贸n:
```
make policy_gradient
```
Este m茅todo guarda los valores de cada juego en un archivo .csv y va guardando gifs cada vez que se alcanza un record de recomensas, quedan en la carpeta scores y gifs respectivamente.

## Desaf铆os de preparaci贸n y homologaci贸n 锔 ##
Se investigan 8 implementaciones realizadas por autores diferentes, de las cuales se seleccionaron 3 las que fueron implementadas con las siguientes consideraciones:
* Habilitaci贸n hardware propio ya que no ejecutan en colab las implementaciones utilizadas, por problemas de versionamiento o interfaz gr谩fica.
* GPU utilizada Nvidia: GTX 1660TI  16GB ram en entorno
* CPU utilizada: intel I9                     16GB ram en entono
*Instalaci贸n y aprendizaje de librer铆as:
Pygame /Pytorch /imageio/scipy.ndimage/Tensorflow/matplotlib.pyplot
Implementaci贸n de entornos virtuales espec铆ficos para cada t茅cnica  ya que las versiones GPU presentaban conflictos de instalaci贸n:
* Se utiliz贸 Anaconda instalando versiones espec铆ficas de cada implementaci贸n.

## M茅tricas de comparaci贸n 锔 ##
Para poder realizar una comparaci贸n efectiva debemos definir un entorno de comparaci贸n justo y modificar las diversas implementaciones para que sean comparables entre ellas, con ese objetivo tomamos las siguientes consideraciones:
Ajustamos el tama帽os del tablero a 30x20 cuadrantes o movimientos en todas las implementaciones
Comparamos mejoras en puntaje obtenido vs cantidad de juegos de entrenamiento para definir qu茅 algoritmo aprende m谩s r谩pido comparando puntaje promedio y m谩ximo.

Incorporamos datos de un humano aprendiendo en condiciones similares como referencia
Adicionalmente, comparamos el tiempo de implementaci贸n, ya que realizar un juego demora tiempos distintos dependiendo del algoritmo utilizado.

Finalmente, preparamos versiones distribu铆das utilizando CUDA y versiones no distribuidas usando CPU para comparar los beneficios en t茅rminos de ahorro de tiempo de entrenamiento, sin embargo, el c贸digo presentado en este repositorio solo presenta la implementaci贸n en CPU.

## Conclusiones  ##

La t茅cnica que logra el mejor desempe帽os es Deep Q-learning , sin embargo, t茅cnicas m谩s antiguas como q learning tradicional  logran un resultado muy cercano.
La t茅cnica que aprende m谩s r谩pido es el Q-learning tradicional y la m谩s lenta (hasta 4 veces m谩s lenta) es Policy gradient  posiblemente porque no actualiza la pol铆tica hasta que la serpiente muere, sin embargo, es la que aprende de forma m谩s estable, debido a que la forma de actualizaci贸n de la pol铆tica asegura de mejor forma ejecutar los movimientos que generen la mayor recompensa.

En uso de recursos computacionales policy gradient es la t茅cnica m谩s pesada con un uso de recursos 3 a 4 veces mayor que las otras 2 t茅cnicas, posiblemente porque en la medida que va aprendiendo la funci贸n objetivo se vuelve m谩s compleja.

Respecto a la facilidad de implementaci贸n, son similares ya que la mayor dificultad est谩 en programar un entorno que refleje fielmente el problema que se quiere resolver.
Todos los algoritmos analizados logran un nivel de desempe帽o que supera a un ser humano, generando jugadas dif铆cilmente igualables por un ser humano, sin embargo, el ser humano es capaz de aprender m谩s r谩pido y con menos ejemplos.




